"use client"

import { useState, useEffect, useRef, useCallback } from "react"
import { Button } from "@/components/ui/button"
import { Avatar } from "@/components/avatar"
import type { ConversationMessage } from "@/app/page"
import { Square } from "lucide-react"

interface ConversationModeProps {
  onEndSession: (history: ConversationMessage[]) => void
}

export function ConversationMode({ onEndSession }: ConversationModeProps) {
  const [avatarState, setAvatarState] = useState<"idle" | "listening" | "talking">("idle")
  const [isRecording, setIsRecording] = useState(false)
  const [transcript, setTranscript] = useState("")
  const [aiResponse, setAiResponse] = useState("")
  const [conversationHistory, setConversationHistory] = useState<ConversationMessage[]>([])
  const [error, setError] = useState<string | null>(null)

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const synthesisRef = useRef<SpeechSynthesisUtterance | null>(null)

  useEffect(() => {
    return () => {
      // Cleanup: Stop recording and release microphone
      if (mediaRecorderRef.current?.state === 'recording') {
        mediaRecorderRef.current.stop()
      }
      if (mediaRecorderRef.current?.stream) {
        mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop())
      }
      window.speechSynthesis?.cancel()
    }
  }, [])

  const startRecording = useCallback(async () => {
    setError(null)
    setTranscript("")
    audioChunksRef.current = []
    setIsRecording(true)
    setAvatarState("listening")
    
    try {
      console.log("üé§ [Recording] Requesting microphone access...")
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000, // Good quality for speech
        } 
      })
      
      // Check supported mime types
      let mimeType = 'audio/webm'
      if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
        mimeType = 'audio/webm;codecs=opus'
      } else if (MediaRecorder.isTypeSupported('audio/mp4')) {
        mimeType = 'audio/mp4'
      }
      
      console.log("üé§ [Recording] Using mime type:", mimeType)
      
      const mediaRecorder = new MediaRecorder(stream, { mimeType })
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
          console.log("üé§ [Recording] Chunk received:", event.data.size, "bytes")
        }
      }
      
      mediaRecorder.onerror = (event) => {
        console.error("‚ùå [Recording] MediaRecorder error:", event)
        setError("Recording error occurred")
        setIsRecording(false)
        setAvatarState("idle")
      }
      
      mediaRecorder.start(100) // Collect chunks every 100ms
      mediaRecorderRef.current = mediaRecorder
      
      console.log("‚úÖ [Recording] Started successfully")
    } catch (e: any) {
      console.error("‚ùå [Recording] Failed to start:", e)
      setError(`Microphone access denied: ${e.message}`)
      setIsRecording(false)
      setAvatarState("idle")
    }
  }, [])

  const stopRecording = useCallback(async () => {
    setIsRecording(false)
    
    if (!mediaRecorderRef.current || mediaRecorderRef.current.state === 'inactive') {
      console.warn("‚ö†Ô∏è [Recording] MediaRecorder not active")
      setAvatarState("idle")
      return
    }
    
    console.log("üé§ [Recording] Stopping...")
    
    // Stop recording
    mediaRecorderRef.current.stop()
    
    // Wait for final data chunk and stop event
    await new Promise<void>((resolve) => {
      if (mediaRecorderRef.current) {
        mediaRecorderRef.current.onstop = () => {
          console.log("üé§ [Recording] Stopped. Total chunks:", audioChunksRef.current.length)
          resolve()
        }
      }
    })
    
    // Create audio blob from chunks
    const audioBlob = new Blob(audioChunksRef.current, { 
      type: mediaRecorderRef.current.mimeType 
    })
    
    console.log("üìÅ [Recording] Audio blob created:", {
      size: audioBlob.size,
      type: audioBlob.type,
      sizeInMB: (audioBlob.size / 1024 / 1024).toFixed(2) + " MB"
    })
    
    // Stop microphone stream
    if (mediaRecorderRef.current.stream) {
      mediaRecorderRef.current.stream.getTracks().forEach(track => {
        track.stop()
        console.log("üé§ [Recording] Microphone track stopped")
      })
    }
    
    if (audioBlob.size === 0) {
      console.error("‚ùå [Recording] Audio blob is empty")
      setError("No audio recorded. Please try again.")
      setAvatarState("idle")
      return
    }
    
    setAvatarState("talking")
    
    try {
      // Send audio to STT API
      const formData = new FormData()
      formData.append('audio', audioBlob, 'recording.webm')
      
      console.log("üì§ [STT] Sending audio for transcription...")
      
      const sttResponse = await fetch('/api/stt', {
        method: 'POST',
        body: formData,
      })
      
      if (!sttResponse.ok) {
        throw new Error(`STT API error: ${sttResponse.status}`)
      }
      
      const sttData = await sttResponse.json()
      const transcriptText = sttData.transcript || ""
      
      console.log("‚úÖ [STT] Received transcript:", transcriptText)
      setTranscript(transcriptText)
      
      if (!transcriptText.trim()) {
        setError("No speech detected in audio")
        setAvatarState("idle")
        return
      }
      
      console.log("üìù [User Input] Transcript:", transcriptText)
      
      const userMessage: ConversationMessage = {
        role: "user",
        content: transcriptText, // Use transcriptText from STT
        timestamp: new Date(),
      }
      
      // Fix race condition: Update history first and use the updated value
      const updatedHistory = [...conversationHistory, userMessage]
      setConversationHistory(updatedHistory)
      
      console.log("üìö [Conversation History] Updated:", updatedHistory.map(m => ({
        role: m.role,
        content: m.content.substring(0, 50) + (m.content.length > 50 ? '...' : '')
      })))

      // Continue with chat API request
      const requestPayload = {
        message: transcriptText, // Use transcriptText from STT
        history: updatedHistory,
      }
        
        console.log("üì§ [API Request] Sending to /api/chat:", {
          message: requestPayload.message,
          historyLength: requestPayload.history.length,
          lastMessages: requestPayload.history.slice(-3).map(m => ({
            role: m.role,
            content: m.content.substring(0, 50) + '...'
          }))
        })
        
        const response = await fetch("/api/chat", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify(requestPayload),
        })

        const data = await response.json()
        console.log("üì• [API Response] Received:", {
          status: response.status,
          response: data.response?.substring(0, 100) + (data.response?.length > 100 ? '...' : ''),
          fullResponse: data.response
        })
        
        const aiText = data.response || "I'm sorry, I didn't catch that. Could you repeat?"
        setAiResponse(aiText)

        const aiMessage: ConversationMessage = {
          role: "assistant",
          content: aiText,
          timestamp: new Date(),
        }
        setConversationHistory((prev) => [...prev, aiMessage])
        
        console.log("ü§ñ [AI Response] Set:", aiText)

        speakResponse(aiText)
      } catch (e) {
        console.error("‚ùå [Chat API] Failed to get AI response:", e)
        setAiResponse("Sorry, I encountered an error. Please try again.")
        setAvatarState("idle")
      }
    } catch (e) {
      console.error("‚ùå [STT] Transcription failed:", e)
      setError("Failed to transcribe audio. Please try again.")
      setAvatarState("idle")
    }
  }, [conversationHistory])

  const speakResponse = async (text: string) => {
    try {
      // Call ElevenLabs TTS API
      const response = await fetch("/api/tts", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text }),
      })

      if (!response.ok) {
        throw new Error("TTS API failed")
      }

      const data = await response.json()
      
      // Convert base64 to audio and play
      const audio = new Audio(`data:audio/mpeg;base64,${data.audio}`)
      audio.onended = () => {
        setAvatarState("idle")
        setTranscript("")
      }
      audio.onerror = () => {
        console.error("Audio playback error")
        setAvatarState("idle")
      }
      await audio.play()
    } catch (error) {
      console.error("Failed to speak:", error)
      // Fallback to Web Speech API
      if ("speechSynthesis" in window) {
        window.speechSynthesis.cancel()
        const utterance = new SpeechSynthesisUtterance(text)
        utterance.rate = 0.9
        utterance.pitch = 1
        utterance.onend = () => {
          setAvatarState("idle")
          setTranscript("")
        }
        synthesisRef.current = utterance
        window.speechSynthesis.speak(utterance)
      } else {
        setAvatarState("idle")
      }
    }
  }

  const handleEndSession = () => {
    // Stop recording if active
    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.stop()
    }
    // Stop microphone
    if (mediaRecorderRef.current?.stream) {
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop())
    }
    // Stop speech synthesis
    window.speechSynthesis?.cancel()
    
    onEndSession(conversationHistory)
  }

  return (
    <div className="gradient-bg flex flex-col items-center min-h-svh px-6 py-8 md:py-12">
      {/* End Session Button - minimal, top right */}
      <div className="w-full max-w-sm md:max-w-md lg:max-w-lg flex justify-end mb-8 md:mb-12">
        <Button
          variant="ghost"
          size="sm"
          onClick={handleEndSession}
          className="text-muted-foreground hover:text-foreground hover:bg-white/50 rounded-full px-4"
        >
          <Square className="w-3 h-3 mr-2" />
          End
        </Button>
      </div>

      {/* Avatar - central focus */}
      <div className="flex-1 flex items-center justify-center">
        <Avatar state={avatarState} size="lg" />
      </div>

      {/* Transcript Display - minimal, only when active */}
      <div className="w-full max-w-sm md:max-w-md lg:max-w-lg mb-8 min-h-[60px] md:min-h-[80px]">
        {error && <div className="p-4 rounded-2xl bg-destructive/10 text-destructive text-sm text-center">{error}</div>}

        {!error && (transcript || aiResponse) && (
          <p className="text-center text-foreground/80 text-sm md:text-base leading-relaxed px-4">
            {avatarState === "listening" && transcript}
            {avatarState === "talking" && aiResponse}
          </p>
        )}
      </div>

      {/* Recording Control - single elegant button */}
      <div className="w-full max-w-sm md:max-w-md">
        <Button
          onClick={isRecording ? stopRecording : startRecording}
          size="lg"
          className={cn(
            "w-full h-14 md:h-16 text-base md:text-lg font-medium rounded-full transition-all duration-300 shadow-lg hover:shadow-xl hover:scale-[1.02] active:scale-[0.98]",
            isRecording ? "bg-primary text-primary-foreground" : "bg-foreground text-background hover:bg-foreground/90",
          )}
        >
          {isRecording ? "Tap to Send" : "Tap to Speak"}
        </Button>
      </div>
    </div>
  )
}

function cn(...classes: (string | boolean | undefined)[]) {
  return classes.filter(Boolean).join(" ")
}
